# -*- coding: utf-8 -*-
"""Supervised_unsupervised_K_means_SMOTE

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XfZe5Vzs4DdLHLdcbsFn4oNIVlgpI0ui
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import numpy as np

from sklearn.linear_model import Perceptron
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

df_lab_q= pd.read_excel('/content/drive/My Drive/Colab Notebooks/laboratory/lab_q_data.xlsx')

df_lab_q.columns

df_lab_q['Diabetes'].value_counts() #number of 1's is too low, thus the value of 3 and 9 will be replaced.

df_lab_q['Diabetes'] = df_lab_q['Diabetes'].replace(3,1)
df_lab_q['Diabetes'] = df_lab_q['Diabetes'].replace(9,1)

df_lab_q['Diabetes'].value_counts()

#df_lab_q.info() #there are come columns with less than 3000 as the value. those rows will be dropped.
import matplotlib.pyplot as plt

# Get counts
counts = df_lab_q['Diabetes'].value_counts()

# Create pie chart
plt.figure(figsize=(6,6))
plt.pie(counts, labels=counts.index.astype(str), autopct='%1.1f%%')
plt.title("Distribution of Diabetes Classes")
plt.show()

df_lab_q = df_lab_q.loc[:, df_lab_q.count() >= 3000] #kept all the columns whose count was greater than 3000
df_lab_q.info()

"""Questions to Ask:
1. How much missing data?
2. What are the outliers?
3. Any wrong data types?
4. Do Any Columns make sense physically?




"""

df_lab_q.describe()
#df_lab_q.isna().sum()

"""Looking into each column and fining outliers:
1) SEQN not needed in ML
2) Fasting Glucose: outlier possibility : 561 healthy: 70-100, diabetec: 125+ >300 suspicious
3) insulin:>100 biologically extreme, need to be double checked.
4) albumin: heavy tail distribution ( use tranformation here, preferrably log)
5) glycohaemoglobin: 17 high but possible
6) diabetes:CATEGORICAL
7) highBEever: CATEGORICAL
8) highCholEver:CATEGORICAL
9) OnCholMeds:CATEGORICAL
10) Diabetes Risk Flag: CATEGORICAL
11) Freq Moderate Activity: 9999 (sentinel value) here don't know from the data and 7777 refused to answer
12) Freq Vigorous Activity: 9999( sentinel value)
13) Minutes Sedantary : 9999 ( sentinel value)
"""

# check the counts for the sentinel values, clearly 9999 is an outlier, replace it with nan, considering the description that none go to such high values. #10,11,12 resolved
df_lab_q.replace(9999, np.nan, inplace=True)
df_lab_q.replace(7777, np.nan, inplace=True)

df_lab_q.drop('SEQN', axis=1, inplace=True) #1 resolved

df_lab_q.describe(percentiles=[.95, .99])

"""From above, we see that, fasting glucose, insulin, glycohaemogkobin and albumin have a possibility of being outliers. These are all valid data.
1) Plasma Fasting Glucose: 59 to 561
2) Insulin: however the range is : 0.35 to 699.9
3) glycohaemoglobin: 3.2 to 17.1
4) Albumin: 0.01 to 7942.47
To process each, lets look at the distribution of data, then we can apply our machine learning algorithms here.
"""

df_lab_q.hist(bins=50, figsize=(20,15))

df_lab_q['Fasting_Glucose_mg/DL'].hist(bins=50, figsize=(20,15))

"""From these we can see that insulin and albumin are the worst right skewed data, and thus we are going to uselog function to process the data and remove the outliers to further clean the data.

"""

df_lab_q['Insulin_log'] = np.log1p(df_lab_q['Insulin_uU/mL']) #compresses huge values
df_lab_q['Albumin_log'] = np.log1p(df_lab_q['Albumin_urine_ug/mL']) #compresses huge values

#df_lab_q[["Insulin_log", "Albumin_log"]].hist(bins=50)

#fasting glucose still has a strong right tail, so what we can do is use power transform to make the data more gaussian like
from sklearn.preprocessing import PowerTransformer

pt = PowerTransformer(method="yeo-johnson")
df_lab_q["Glucose_pt"] = pt.fit_transform(df_lab_q[["Fasting_Glucose_mg/DL"]])
#df_lab_q[["Glucose_pt"]].hist(bins=50)

df_lab_q["Minutes_Sedentary_log"] = np.log1p(df_lab_q["Minutes_Sedentary"])
df_lab_q["Freq_Mod_log"] = np.log1p(df_lab_q["Freq_Moderate_Activity"])
df_lab_q["Freq_Vig_log"] = np.log1p(df_lab_q["Freq_Vigorous_Activity"])
#df_lab_q[["Minutes_Sedentary_log"]].hist(bins=50)

drop_cols = ["Insulin_uU/mL", "Albumin_urine_ug/mL", "Fasting_Glucose_mg/DL","Minutes_Sedentary","Freq_Moderate_Activity","Freq_Vigorous_Activity"]
df_clean = df_lab_q.drop(columns=drop_cols)
df_clean.info()

import seaborn as sns
corr_matrix = df_clean.corr()
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')

df_clean = df_clean.apply(lambda col: col.fillna(col.median())) # now that the ourliers are removed fill the missing values with median.

"""Data Splitting:
1) Splitting the data into training and test samples.
2) Balancing the data using an oversampling method ADASYN.
3) Using Stratify for the y value to fairly represent both the test and the train data.
4) Change the label of the bins for the y data, 0 is for the negative class ( Not Having Diabetes) and 1 is for the positive class( Having Diabetes) This is used for the ROC curve.
"""

from imblearn.over_sampling import KMeansSMOTE
from imblearn.over_sampling import ADASYN
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import cross_val_score, StratifiedKFold
from imblearn.pipeline import Pipeline


X = df_clean.drop("Diabetes", axis=1)
y = df_clean["Diabetes"]
#split the training and the test data
X_train, X_test, y_tr, y_te = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)
#change the label of bins
Y_train = (y_tr==1). astype(int)
Y_test = (y_te==1).astype(int)

#calculate the metrics
model_comparison_list = []  # Store results for all models

from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score

def calculate_metrics(model_name, y_true, y_pred):
    # Create a dictionary for this model
    metrics_dict = {
        'Model': model_name,
        'Accuracy': accuracy_score(y_true, y_pred),
        'Recall': recall_score(y_true, y_pred),
        'Precision': precision_score(y_true, y_pred),
        'F1': f1_score(y_true, y_pred)
    }

    # Append to the list
    model_comparison_list.append(metrics_dict)

    # Print metrics
    print(f"ML Model: {model_name}")
    print("Accuracy score: ", metrics_dict['Accuracy'])
    print("Recall score: ", metrics_dict['Recall'])
    print("Precision score: ", metrics_dict['Precision'])
    print("F1 score: ", metrics_dict['F1'])

from sklearn.metrics import roc_curve, auc
from matplotlib import pyplot as plt
def ROC_curve_predict_proba(clf,X_test,y_true):
    y_scores = clf.predict_proba(X_test)[:, 1]
    fpr, tpr, thresholds = roc_curve(y_true, y_scores)
    roc_auc = auc(fpr, tpr)
    plt.figure()
    plt.plot(fpr, tpr, label=f"AUC = {roc_auc:.2f}", linewidth=2)
    plt.plot([0,1], [0,1], '--', color='gray')
    plt.xlabel("False Positive Rate")
    plt.ylabel("True Positive Rate (Recall)")
    plt.title("ROC Curve Logistic Regression")
    plt.legend(loc="lower right")
    plt.grid(True)
    plt.show()

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, roc_curve, auc
def Confusion_matrix(y_true, y_pred):
    ConfusionMatrixDisplay.from_predictions(y_true, y_pred)

from sklearn.metrics import roc_curve, auc
from matplotlib import pyplot as plt
def ROC_curve_decision_function(model, X_test, y_true):
    y_scores = model.decision_function(X_test)
    fpr, tpr, thresholds = roc_curve(y_true, y_scores)
    roc_auc = auc(fpr, tpr)
    # Plot
    plt.figure()
    plt.plot(fpr, tpr, label=f"AUC = {roc_auc:.2f}", linewidth=2)
    plt.plot([0, 1], [0, 1], '--', color='gray')
    plt.xlabel("False Positive Rate")
    plt.ylabel("True Positive Rate (Recall)")
    plt.title(f"ROC Curve ({model.__class__.__name__})")
    plt.legend(loc="lower right")
    plt.grid(True)
    plt.show()

"""Using Perceptron to train and test the model."""

from sklearn.linear_model import Perceptron
pe_model = Pipeline([('scaler',StandardScaler()),('adasyn', ADASYN()),
 ('pe_model', Perceptron(max_iter=3000, class_weight='balanced',random_state=42))])

cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
cv_scores = cross_val_score(pe_model, X_train, Y_train, cv=cv, scoring='f1')

print("CV F1 scores:", cv_scores)
print("Mean CV F1:", cv_scores.mean())
pe_model.fit(X_train, Y_train)
pe_y_pred = pe_model.predict(X_test)

Confusion_matrix(Y_test, pe_y_pred)
#calculate_metrics("Perceptron",Y_test, pe_y_pred)
ROC_curve_decision_function(pe_model,X_test,Y_test)

from sklearn.linear_model import Perceptron
pe_model = Pipeline([('scaler',StandardScaler()),('kmeanssmote', KMeansSMOTE(random_state=42)),
 ('pe_model', Perceptron(max_iter=3000,random_state=42))])

cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
cv_scores = cross_val_score(pe_model, X_train, Y_train, cv=cv, scoring='f1')

print("CV F1 scores:", cv_scores)
print("Mean CV F1:", cv_scores.mean())
pe_model.fit(X_train, Y_train)
pe_y_pred = pe_model.predict(X_test)

Confusion_matrix(Y_test, pe_y_pred)
calculate_metrics("Perceptron", Y_test, pe_y_pred)
ROC_curve_decision_function(pe_model,X_test,Y_test)

import shap
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# 1. Prediction function (important fix)
def model_predict(X):
    X_df = pd.DataFrame(X, columns=X_train.columns)
    return pe_model.decision_function(X_df)

# 2. Sample background correctly
background = X_train.iloc[np.random.choice(X_train.shape[0], 50, replace=False)]

# 3. Create SHAP explainer
explainer = shap.KernelExplainer(model_predict, background)

# 4. Compute SHAP values
shap_values = explainer.shap_values(X_test)

# 5. Summary plot
shap.summary_plot(shap_values, X_test, feature_names=X_train.columns)

"""Logistic Regression Model with L2(Ridge) penalty."""

from sklearn.linear_model import LogisticRegression

lr_model = Pipeline([('scaler',StandardScaler()),('kmeanssmote', KMeansSMOTE( random_state=42)),
 ('lr_model', LogisticRegression(random_state=42, penalty='l2', max_iter=3000))])


cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
cv_scores = cross_val_score(lr_model, X_train, Y_train, cv=cv, scoring='f1')

print("CV F1 scores:", cv_scores)
print("Mean CV F1:", cv_scores.mean())
lr_model.fit(X_train, Y_train)

lr_y_pred = lr_model.predict(X_test)
Confusion_matrix(Y_test, lr_y_pred)
calculate_metrics("Logistic_Regression",Y_test, lr_y_pred)
ROC_curve_predict_proba(lr_model,X_test,Y_test)

"""Support Vector Machine with a linear Kernel"""

from sklearn import svm

lsvm_model = Pipeline([('scaler',StandardScaler()),('kmeanssmote', KMeansSMOTE(random_state=42)),
 ('lsvm_model', svm.SVC(kernel='linear', random_state=42))])

cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
cv_scores = cross_val_score(lsvm_model, X_train, Y_train, cv=cv, scoring='f1')
lsvm_model.fit(X_train, Y_train)
lsvm_y_pred = lsvm_model.predict(X_test)


print("CV F1 scores:", cv_scores)
print("Mean CV F1:", cv_scores.mean())
Confusion_matrix(Y_test, lsvm_y_pred)
calculate_metrics("SVM",Y_test, lsvm_y_pred)

"""K-nearest Neightbour"""

from sklearn.neighbors import KNeighborsClassifier

knn_model= Pipeline([('scaler',StandardScaler()),('kmeanssmote', KMeansSMOTE(random_state=42)),
 ('knn_model', KNeighborsClassifier(n_neighbors=5))])
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
cv_scores = cross_val_score(knn_model, X_train, Y_train, cv=cv, scoring='f1')

knn_model.fit(X_train, Y_train)
knn_y_pred = knn_model.predict(X_test)
print("CV F1 scores:", cv_scores)
print("Mean CV F1:", cv_scores.mean())
Confusion_matrix(Y_test, knn_y_pred)
calculate_metrics("K-Nearest Neighbor",Y_test, knn_y_pred)

from sklearn.neural_network import MLPClassifier

mlp_model = Pipeline([('scaler',StandardScaler()),('kmeanssmote', KMeansSMOTE(random_state=42)),
 ('mlp_model', MLPClassifier(hidden_layer_sizes=(16,8), activation='logistic',solver='adam',max_iter=3000, random_state=42))])

cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
cv_scores = cross_val_score(mlp_model, X_train, Y_train, cv=cv, scoring='f1')

mlp_model.fit(X_train, Y_train)
mlp_y_pred = mlp_model.predict(X_test)

print("CV F1 scores:", cv_scores)
print("Mean CV F1:", cv_scores.mean())
Confusion_matrix(Y_test, mlp_y_pred)
calculate_metrics("Neural Network Logistic",Y_test, mlp_y_pred)
ROC_curve_predict_proba(mlp_model,X_test,Y_test)

import shap
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# ----------------------------------------------------
# 1. Prediction function for SHAP
#    (SHAP sends numpy arrays → convert to DataFrame → pipeline stays happy)
# ----------------------------------------------------
def model_predict(X):
    X_df = pd.DataFrame(X, columns=X_train.columns)
    # For classifiers with no decision_function, use predict_proba
    return knn_model.predict_proba(X_df)[:, 1]  # SHAP expects 1D for binary classification

# ----------------------------------------------------
# 2. Background sample (needed for Kernel SHAP)
# ----------------------------------------------------
background = X_train.iloc[np.random.choice(X_train.shape[0], 50, replace=False)]

# ----------------------------------------------------
# 3. Create Kernel Explainer
# ----------------------------------------------------
explainer = shap.KernelExplainer(model_predict, background)

# ----------------------------------------------------
# 4. Compute SHAP values on test data
# ----------------------------------------------------
# Optional: use first 200 rows to make it faster
X_test_sample = X_test.iloc[:200]

shap_values = explainer.shap_values(X_test_sample)

# ----------------------------------------------------
# 5. SHAP Summary plot
# ----------------------------------------------------
shap.summary_plot(shap_values, X_test_sample, feature_names=X_train.columns)

from sklearn.neural_network import MLPClassifier

mlpr_model = Pipeline([('scaler',StandardScaler()),('kmeanssmote', KMeansSMOTE(random_state=42)),
 ('mlpr_model', MLPClassifier(hidden_layer_sizes=(16,8), activation='relu',solver='adam',max_iter=3000, random_state=42))])

cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
cv_scores = cross_val_score(mlpr_model, X_train, Y_train, cv=cv, scoring='f1')

mlpr_model.fit(X_train, Y_train)
mlpr_y_pred = mlpr_model.predict(X_test)
Confusion_matrix(Y_test, mlpr_y_pred)
calculate_metrics("Neural Network ReLU",Y_test, mlpr_y_pred)

from sklearn import tree
dt_model = tree.DecisionTreeClassifier(random_state=42)
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
dt_model.fit(X_train, Y_train)
dt_y_pred = dt_model.predict(X_test)
Confusion_matrix(Y_test, dt_y_pred)
calculate_metrics("Decision Tree",Y_test, dt_y_pred)

import pandas as pd

# Convert list of metric dictionaries to a DataFrame
metrics_df = pd.DataFrame(model_comparison_list)


metrics_df.sort_values(by='F1', ascending=False, inplace=True)
#print(metrics_df)
# Round for readability
metrics_rounded = metrics_df.round(3)
metrics_rounded.index.name = "ML_Model"

import matplotlib.pyplot as plt

# Use the rounded DataFrame without the index numbers
metrics_display = metrics_rounded.reset_index()  # Model becomes a column

fig, ax = plt.subplots(figsize=(8, 2))
ax.axis('tight')
ax.axis('off')

# Only use the values for cellText (this excludes the default row numbers)
table = ax.table(cellText=metrics_display.values,
                 colLabels=metrics_display.columns,
                 cellLoc='center',
                 loc='center')

table.auto_set_font_size(False)
table.set_fontsize(10)
table.auto_set_column_width(col=list(range(len(metrics_display.columns))))

# Save the figure without row numbers
plt.savefig("model_metrics_table.png", bbox_inches='tight', dpi=300)
plt.show()

from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline

kmeans_model = Pipeline([
    ('scaler', StandardScaler()),
    ('kmeans', KMeans(n_clusters=2, random_state=42))
])

clusters = kmeans_model.fit_predict(X)
df_lab_q['Cluster'] = clusters
pd.crosstab(df_lab_q['Cluster'], df_lab_q['Diabetes'])



from sklearn.cluster import DBSCAN

dbscan = Pipeline([
    ('scaler', StandardScaler()),
    ('dbscan', DBSCAN(eps=0.5, min_samples=15))
])

labels = dbscan.fit_predict(X)
df_lab_q['DBSCAN_Cluster'] = labels
pd.crosstab(df_lab_q['DBSCAN_Cluster'], df_lab_q['Diabetes'])

from sklearn.mixture import GaussianMixture
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

gmm = GaussianMixture(n_components=3, covariance_type='full', random_state=42)
clusters = gmm.fit_predict(X_scaled)

df_lab_q['GMM_Cluster'] = clusters
pd.crosstab(df_lab_q['GMM_Cluster'], df_lab_q['Diabetes'])

#create anomalies mask
from sklearn.ensemble import IsolationForest

iso = IsolationForest(
    n_estimators=200,
    contamination=0.05,   # assume ~5% anomalies (tuneable)
    random_state=42
)

iso.fit(X_train)
train_iso_labels = iso.predict(X_train)   # -returns a 1-D array with the 1 as the normal value and -1 for the anomaly for each training sample.

#create the mask
len(train_iso_labels)
mask = train_iso_labels == 1 #creates the boolean mask to discard the anomalies
X_train_filtered = X_train[mask] #create a new dataset with the removed anomalies.
Y_train_filtered = Y_train[mask]
print("Original training samples:", len(X_train))
print("After removing anomalies:", len(X_train_filtered))
lr_model.fit(X_train_filtered, Y_train_filtered) #use the filtered dataset to use the logistic regression.

lr_y_pred = lr_model.predict(X_test)

Confusion_matrix(Y_test, lr_y_pred)
calculate_metrics("LR_IsolationFiltered", Y_test, lr_y_pred)
ROC_curve_predict_proba(lr_model, X_test, Y_test)

train_scores = iso.decision_function(X_train)
test_scores = iso.decision_function(X_test)

X_train_aug = np.column_stack([X_train, train_scores])
X_test_aug  = np.column_stack([X_test, test_scores])
lr_model.fit(X_train_aug, Y_train)

lr_y_pred = lr_model.predict(X_test_aug)

calculate_metrics("LR_With_AnomalyScore", Y_test, lr_y_pred)
import pandas as pd

df_anomaly = pd.DataFrame({
    "Anomaly": iso.predict(X_test),
    "True_Label": Y_test
})

pd.crosstab(df_anomaly["Anomaly"], df_anomaly["True_Label"])